{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9GXipPr89BGI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn.functional import log_softmax\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdY-Foly9XkR",
        "outputId": "bb61be49-9f12-442e-ed55-c49fac5bed27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6O3YOOY_AN0"
      },
      "outputs": [],
      "source": [
        "class CRNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CRNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv_1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)  # (16, 64)\n",
        "\n",
        "        self.conv_2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)  # (8, 32)\n",
        "\n",
        "        self.conv_3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.pool_4 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))  # (4, 32)\n",
        "\n",
        "        self.conv_5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.batch_norm_5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv_6 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.batch_norm_6 = nn.BatchNorm2d(512)\n",
        "        self.pool_6 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))  # (2, 32)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm_1 = nn.LSTM(input_size=1024, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.lstm_2 = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # Final output layer (dense layer)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv_1(x))\n",
        "        x = self.pool_1(x)\n",
        "\n",
        "        x = F.relu(self.conv_2(x))\n",
        "        x = self.pool_2(x)\n",
        "\n",
        "        x = F.relu(self.conv_3(x))\n",
        "        x = F.relu(self.conv_4(x))\n",
        "        x = self.pool_4(x)\n",
        "\n",
        "        x = F.relu(self.conv_5(x))\n",
        "        x = self.batch_norm_5(x)\n",
        "\n",
        "        x = F.relu(self.conv_6(x))\n",
        "        x = self.batch_norm_6(x)\n",
        "        x = self.pool_6(x)\n",
        "\n",
        "        # Reshape for LSTM input\n",
        "        batch_size, channels, height, width = x.size()\n",
        "\n",
        "        # Permute and reshape to match LSTM input requirements\n",
        "        x = x.permute(0, 3, 1, 2)  \n",
        "        x = x.contiguous().view(batch_size, width, height * channels) \n",
        "\n",
        "        if x.size(-1) != 1024:\n",
        "            raise ValueError(f\"Expected input size of 1024, but got {x.size(-1)}\")\n",
        "\n",
        "        x, _ = self.lstm_1(x)\n",
        "        x, _ = self.lstm_2(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CTC Loss and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgERyZ2D_BUE"
      },
      "outputs": [],
      "source": [
        "# Define the loss function (CTC Loss)\n",
        "def ctc_loss(pred, target, input_lengths, target_lengths):\n",
        "    # CTC Loss expects input in shape (seq_len, batch, num_classes)\n",
        "    return nn.CTCLoss(blank=0, zero_infinity=True)(pred, target, input_lengths, target_lengths)\n",
        "\n",
        "# Define the Dataset class to load your PNG images and their corresponding text\n",
        "class OCRDataset(Dataset):\n",
        "    def __init__(self, image_folder, char_list, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.char_list = char_list\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
        "\n",
        "        # Convert the text label (image name without extension)\n",
        "        label = os.path.basename(img_path).split('.')[0]\n",
        "\n",
        "        # Create target tensor for CTC loss (mapping characters to indices)\n",
        "        target = torch.tensor([self.char_list.index(c) for c in label], dtype=torch.int)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        img_width, img_height = img.shape[1], img.shape[2]\n",
        "        return img, target, torch.tensor([img_width // 4], dtype=torch.int), torch.tensor([len(label)], dtype=torch.int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KMmFMX9_EZ-"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    input_lengths = []\n",
        "    target_lengths = []\n",
        "\n",
        "    for img, target, input_len, target_len in batch:\n",
        "        images.append(img)\n",
        "        targets.append(target)\n",
        "        input_lengths.append(input_len)\n",
        "        target_lengths.append(target_len)\n",
        "\n",
        "    targets = pad_sequence(targets, batch_first=True, padding_value=0)  \n",
        "\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    input_lengths = torch.tensor(input_lengths, dtype=torch.int)\n",
        "    target_lengths = torch.tensor(target_lengths, dtype=torch.int)\n",
        "\n",
        "    return images, targets, input_lengths, target_lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKRrDN___Hij"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "image_folder = '/content/dataset_1/dataset_1'  \n",
        "char_list = [''] + list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')  \n",
        "num_classes = len(char_list) \n",
        "\n",
        "# Data Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = OCRDataset(image_folder, char_list, transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "model = CRNN(num_classes).to(device)  \n",
        "optimizer = Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Greedy Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBM9c2y5CTbI"
      },
      "outputs": [],
      "source": [
        "def decode_predictions(output, char_list, blank_index=0):  \n",
        "    decoded_preds = []\n",
        "    for i in range(output.size(1)): \n",
        "        pred_indices = torch.argmax(output[:, i, :], dim=1).tolist()  \n",
        "        pred_text = []\n",
        "        previous_char = None\n",
        "        for index in pred_indices:\n",
        "            if index != previous_char and index != blank_index:  \n",
        "                pred_text.append(char_list[index])\n",
        "            previous_char = index\n",
        "        decoded_preds.append(''.join(pred_text))\n",
        "    return decoded_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AhS5Jcs_6KT",
        "outputId": "c6757e83-5b59-4d81-fc45-c2ccc2d05d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Training Loss: 4.2398\n",
            "Target: oLem, Predicted: \n",
            "Target: YhrSejfZ, Predicted: \n",
            "Target: qSHMvZ, Predicted: \n",
            "Epoch 1, Validation Loss: 4.6754, Character Accuracy: 0.0000\n",
            "Epoch 2, Training Loss: 4.0724\n",
            "Target: oLem, Predicted: lc\n",
            "Target: YhrSejfZ, Predicted: NcgG\n",
            "Target: qSHMvZ, Predicted: dG\n",
            "Epoch 2, Validation Loss: 3.2918, Character Accuracy: 0.0383\n",
            "Epoch 3, Training Loss: 1.3585\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: dSHMv\n",
            "Epoch 3, Validation Loss: 0.4771, Character Accuracy: 0.8476\n",
            "Epoch 4, Training Loss: 0.1217\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 4, Validation Loss: 0.0295, Character Accuracy: 0.9234\n",
            "Epoch 5, Training Loss: 0.0522\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 5, Validation Loss: 0.1194, Character Accuracy: 0.8907\n",
            "Epoch 6, Training Loss: 0.0367\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 6, Validation Loss: 0.0493, Character Accuracy: 0.9232\n",
            "Epoch 7, Training Loss: 0.0357\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 7, Validation Loss: 0.1209, Character Accuracy: 0.9271\n",
            "Epoch 8, Training Loss: 0.0287\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 8, Validation Loss: 0.0639, Character Accuracy: 0.9219\n",
            "Epoch 9, Training Loss: 0.0208\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 9, Validation Loss: 0.0309, Character Accuracy: 0.9249\n",
            "Epoch 10, Training Loss: 0.0226\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 10, Validation Loss: -0.0114, Character Accuracy: 0.9272\n",
            "Epoch 11, Training Loss: 0.0169\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 11, Validation Loss: 0.1015, Character Accuracy: 0.9149\n",
            "Epoch 12, Training Loss: 0.0198\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 12, Validation Loss: -0.0084, Character Accuracy: 0.9280\n",
            "Epoch 13, Training Loss: 0.0148\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 13, Validation Loss: 0.0172, Character Accuracy: 0.9266\n",
            "Epoch 14, Training Loss: 0.0123\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 14, Validation Loss: 0.1315, Character Accuracy: 0.9141\n",
            "Epoch 15, Training Loss: 0.0125\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 15, Validation Loss: -0.0506, Character Accuracy: 0.9285\n",
            "Epoch 16, Training Loss: 0.0133\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 16, Validation Loss: 0.0248, Character Accuracy: 0.9277\n",
            "Epoch 17, Training Loss: 0.0104\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 17, Validation Loss: 0.0087, Character Accuracy: 0.9277\n",
            "Epoch 18, Training Loss: 0.0102\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 18, Validation Loss: 0.0130, Character Accuracy: 0.9290\n",
            "Epoch 19, Training Loss: 0.0078\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 19, Validation Loss: 0.0254, Character Accuracy: 0.9285\n",
            "Epoch 20, Training Loss: 0.0099\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 20, Validation Loss: 0.0225, Character Accuracy: 0.9266\n",
            "Epoch 21, Training Loss: 0.0110\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 21, Validation Loss: 0.0305, Character Accuracy: 0.9281\n",
            "Epoch 22, Training Loss: 0.0071\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 22, Validation Loss: 0.0111, Character Accuracy: 0.9284\n",
            "Epoch 23, Training Loss: 0.0082\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 23, Validation Loss: 0.0049, Character Accuracy: 0.9286\n",
            "Epoch 24, Training Loss: 0.0078\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 24, Validation Loss: 0.0168, Character Accuracy: 0.9285\n",
            "Epoch 25, Training Loss: 0.0073\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 25, Validation Loss: 0.0245, Character Accuracy: 0.9282\n",
            "Epoch 26, Training Loss: 0.0073\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 26, Validation Loss: 0.0241, Character Accuracy: 0.9286\n",
            "Epoch 27, Training Loss: 0.0069\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 27, Validation Loss: 0.0467, Character Accuracy: 0.9281\n",
            "Epoch 28, Training Loss: 0.0056\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 28, Validation Loss: 0.0688, Character Accuracy: 0.9281\n",
            "Epoch 29, Training Loss: 0.0074\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 29, Validation Loss: 0.0027, Character Accuracy: 0.9286\n",
            "Epoch 30, Training Loss: 0.0058\n",
            "Target: oLem, Predicted: oLem\n",
            "Target: YhrSejfZ, Predicted: YhrSejf\n",
            "Target: qSHMvZ, Predicted: qSHMv\n",
            "Epoch 30, Validation Loss: 0.0011, Character Accuracy: 0.9284\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(30):  \n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for images, targets, _, target_lengths in train_loader:\n",
        "        images, targets, target_lengths = images.to(device), targets.to(device), target_lengths.to(device)  # Move to device\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        batch_size, seq_len, num_classes = output.size()\n",
        "        input_lengths = torch.full((batch_size,), seq_len, dtype=torch.int32).to(device)\n",
        "\n",
        "        # Compute CTC loss\n",
        "        loss = ctc_loss(output.permute(1, 0, 2), targets, input_lengths, target_lengths)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    total_characters = 0\n",
        "    correct_characters = 0\n",
        "    print_count = 0 \n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets, _, target_lengths in val_loader:\n",
        "            images, targets, target_lengths = images.to(device), targets.to(device), target_lengths.to(device)  # Move to device\n",
        "\n",
        "            output = model(images)\n",
        "            batch_size, seq_len, num_classes = output.size()\n",
        "            input_lengths = torch.full((batch_size,), seq_len, dtype=torch.int32).to(device)\n",
        "\n",
        "            # Compute CTC loss\n",
        "            loss = ctc_loss(output.permute(1, 0, 2), targets, input_lengths, target_lengths)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Use decode_predictions function\n",
        "            decoded_preds = decode_predictions(output.permute(1, 0, 2), char_list, blank_index=num_classes-1)\n",
        "            for pred_text, target in zip(decoded_preds, targets):\n",
        "                target_text = ''.join([char_list[c.item()] for c in target if c.item() != 0])  # Remove padding\n",
        "\n",
        "                if print_count < 3:  # Print only first 3 predictions\n",
        "                    print(f\"Target: {target_text}, Predicted: {pred_text}\")\n",
        "                    print_count += 1\n",
        "\n",
        "                # Calculate character-level accuracy\n",
        "                correct_characters += sum(pc == tc for pc, tc in zip(pred_text, target_text))\n",
        "                total_characters += len(target_text)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    character_accuracy = correct_characters / total_characters if total_characters > 0 else 0\n",
        "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}, Character Accuracy: {character_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C8KQv78MV31",
        "outputId": "fefee33f-1dd4-439c-8e17-aa5087f8dca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved after all epochs.\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), 'task-2_data_1.pth')\n",
        "print(\"Model saved after all epochs.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
